{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fruit-Fly-Net demo ðŸ„.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "saKxS86v0Xe5"
      },
      "source": [
        "# Fruit-Fly-Net demo ðŸ„\n",
        "\n",
        "In this notebook we'll show how to use our implementation of the fruit fly network (Fruit-Fly-Net) introduced in [Can a Fruit Fly Learn Word Embeddings](https://arxiv.org/abs/2101.06887) to create and compare word embeddings from Pokedex entries. ðŸ¸ðŸŒ±ðŸ¢ðŸŒŠðŸ¦ŽðŸ”¥\n",
        "\n",
        "Useful links ðŸ‘€:\n",
        "* [paper](https://arxiv.org/pdf/2101.06887.pdf)\n",
        "* [our repo](https://github.com/Ramos-Ramos/fruit-fly-net)\n",
        "* [authors' official repo](https://github.com/bhoov/flyvec)\n",
        "\n",
        "**Disclaimers ðŸš¨:**\n",
        "* we're not the original authors, we just took a crack at implementing it\n",
        "* we haven't tried reproducing their results\n",
        "\n",
        "**Note: This notebook requires a GPU runtime for training** (`Runtime > Change runtime type > Hardware accelerator > GPU`)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTr8t5RYee_r"
      },
      "source": [
        "## How does it work? ðŸ¤”\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BFQSBVz1cLE"
      },
      "source": [
        "Fruit-Fly-Net creates word embeddings by trying to learn the correlations between words and their contexts.\n",
        "\n",
        "Given a vocabulary of $N_{voc}$ tokens, Fruit-Fly-Net takes in a token and its context in the form of a binary input vector $v^A$ of length $2 \\times N_{voc}$, where the first $N_{voc}$ dimensions form a bag-of-words representation of the context words and the remaining $N_{voc}$ dimensions form a one-hot encoding of the target word. These input vectors are created from n-grams (which the authors refer to as w-grams) taken from the training corpus. The center element of each w-gram becomes the target while the surrounding elements comprise the context.\n",
        "\n",
        "<table>\n",
        "  <tr><td colspan=6><center>\"Charizard breathes flames\"</center></td></tr>\n",
        "  <tr>\n",
        "    <td>breathes</td><td>charizard</td><td>flames</td>\n",
        "    <td>breathes</td><td>charizard</td><td>flames</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>0</td><td>1</td><td>1</td>\n",
        "    <td>1</td><td>0</td><td>0</td>\n",
        "  </tr>\n",
        "<table>\n",
        "\n",
        "Fruit-Fly-Net projects this input vector to $K$ dimensions, of which the top $k$ activations are set to 1 while the rest are suppressed to 0. To update the projection weights, Fruit-Fly-Net requires a $2 \\times N_{voc}$-dimensional vector $p$, which is a concatenation of two duplicate vectors of probabilities of each token appearing in the trainset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSU84sDEKk4a"
      },
      "source": [
        "## Installations and imports ðŸ”§\n",
        "**Note**: You'll have to restart the runtime after installing the packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUT7RpN6hehe"
      },
      "source": [
        "pip install -U einops gradio numpy spacy git+https://github.com/Ramos-Ramos/fruit-fly-net"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cuKdIXvfLdOC"
      },
      "source": [
        "!python -m spacy download en_core_web_sm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okpzNyShcZmI"
      },
      "source": [
        "from einops import rearrange\n",
        "import cupy as cp\n",
        "import cupy as xp\n",
        "import numpy as np\n",
        "import gradio as gr\n",
        "import pandas as pd\n",
        "from cupyx.scipy.sparse import csr_matrix, vstack\n",
        "import spacy\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "from collections import Counter, OrderedDict\n",
        "import pickle\n",
        "\n",
        "from fruit_fly_net import FruitFlyNet, bio_hash_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2w4mZ_LW1uxm"
      },
      "source": [
        "## Tokenizing the dataset ðŸ§©"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jC1jT-os252Y"
      },
      "source": [
        "The corpus from which we'll create word embeddings consists of several Pokedex entries. To create word emebddings, we need to get a list of words to begin with. We can do that by tokenizing our corpus, or converting the corpus into a vocabulary of words, or \"tokens\". For Fruit-Fly-Net to work, we also need a list of probabilities for each token."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4M8JgUG3_Jg"
      },
      "source": [
        "We can download our corpus here. It's in the form of a csv, which we can open in Pandas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eY138te32f-g"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/Ramos-Ramos/fruit-fly-net/demo/dex.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0aefiCOpQlf"
      },
      "source": [
        "df = pd.read_csv('dex.csv')\n",
        "print('shape:', df.shape)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzL1i-wzC2nI"
      },
      "source": [
        "To create longer pieces of text, we can combine Pokedex entries coming from the same Pokemon. This list of concatenated entries will be our corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDY_6uVfBxhY"
      },
      "source": [
        "corpus = df.groupby('name').description.apply(' '.join)\n",
        "print('shape:', corpus.shape)\n",
        "corpus.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hukbMuVDCAb"
      },
      "source": [
        "Now we proceed to the actual tokenization. We use SpaCy for this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ec6tul0L1Xn"
      },
      "source": [
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXChHLYVDJcx"
      },
      "source": [
        "Our function for tokenization splits a piece of text into tokens using SpaCy, and ignores tokens that are punctuations, numbers, or stop words. Note that this is different from the tokenization process of the original authors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPCyDdRJn7oA"
      },
      "source": [
        "def create_tokens_from_text(text):\n",
        "  \"\"\"Tokenizes text by:\n",
        "  - splitting with SpaCy\n",
        "  - ignoring punctuations, numbers, and stop words\n",
        "  \"\"\"\n",
        "  return [w.lemma_.lower() for w in nlp(text) \n",
        "          if not w.is_punct and \n",
        "          not w.like_num and \n",
        "          not w.lemma_.lower() in nlp.Defaults.stop_words]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VqdAq6KFVRe"
      },
      "source": [
        "To finally create the vocabulary, we iterate over the corpus and tokenize the texts using our tokenization function. We can then calculate the probabilities of each token, or the percentage of the corpus they composed. We start with an intial $N_{voc}$ of $20,000$ following the authors but our final vocabulary ends up being much smaller (~$6,500$)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fc6VKpMPesKV"
      },
      "source": [
        "tokens = []\n",
        "init_vocabulary_size = 20000\n",
        "batch_size = 100\n",
        "\n",
        "# create tokens\n",
        "for batch_start in tqdm(range(0, len(corpus), batch_size)):\n",
        "  tokens += create_tokens_from_text(\n",
        "      ' '.join(corpus.iloc[batch_start:batch_start+batch_size])\n",
        "  )\n",
        "\n",
        "# clip vocabulary if necessary and calculate probabilities\n",
        "tokens_to_counts = dict(Counter(tokens).most_common(init_vocabulary_size))\n",
        "total_count = sum(tokens_to_counts.values())\n",
        "tokens_to_probabilities = {token : count / total_count for token, count in tokens_to_counts.items()}\n",
        "\n",
        "# finalize vocabulary size, vocabulary and probabilities\n",
        "vocabulary = list(tokens_to_probabilities.keys())\n",
        "probabilities = xp.tile(xp.array(list(tokens_to_probabilities.values())), 2)\n",
        "vocabulary_size = len(tokens_to_counts)\n",
        "\n",
        "print(f'vocabulary size: {vocabulary_size}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZ582HZeLrxZ"
      },
      "source": [
        "## Preparing trainset ðŸ”¨"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aD211QFTKnGd"
      },
      "source": [
        "We have the vocabulary and probabilities, but we still need to create a trainset in the format accepted by Fruit-Fly-Net as described in the \"How does it work ðŸ¤”\" section. We can create two helper functions for creating token ids (unique numbers for each token in our vocabulary) and the actual input training embeddings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqRZYf26JSoR"
      },
      "source": [
        "def create_token_ids_from_text(text, vocabulary):\n",
        "  \"\"\"Creates tokens from text then gets corresponding indices for tokens in the\n",
        "  vocabulary\n",
        "  \"\"\"\n",
        "  tokens = create_tokens_from_text(text)\n",
        "  token_ids = [vocabulary.index(token) for token in tokens if token in vocabulary]\n",
        "  return token_ids"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-qrS6rU9-Zz"
      },
      "source": [
        "def create_training_embeddings_from_token_ids(token_ids, w_gram_size, vocabulary_size):\n",
        "  \"\"\"Creates several w-grams, then creates input training emebddings by having\n",
        "  the middle token be the target the rest be the context\n",
        "  \"\"\"\n",
        "\n",
        "  # create w-grams\n",
        "  w_gram_size = min(w_gram_size, len(token_ids))\n",
        "  middle_idx = w_gram_size//2\n",
        "  w_grams = xp.array(np.lib.stride_tricks.sliding_window_view(token_ids, w_gram_size))\n",
        "  w_grams[:, middle_idx] += vocabulary_size\n",
        "\n",
        "  # create training embeddings\n",
        "  training_embeddings = xp.zeros((w_grams.shape[0], vocabulary_size*2))\n",
        "  training_embeddings[xp.indices(w_grams.shape)[0], w_grams] = 1\n",
        "  training_embeddings = training_embeddings.astype(xp.bool_)\n",
        "\n",
        "  return training_embeddings"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BeHgoFdjLszA"
      },
      "source": [
        "All we have to do now is iterate over our corpus and create the training embeddings. We use a w-gram size of $15$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "omPGzTaEqOK3"
      },
      "source": [
        "w_gram_size = 15\n",
        "\n",
        "training_embeddings = []\n",
        "for text in tqdm(corpus):\n",
        "  token_ids = create_token_ids_from_text(text, vocabulary)\n",
        "  \n",
        "  training_embeddings.append(\n",
        "      csr_matrix(create_training_embeddings_from_token_ids(\n",
        "          token_ids, w_gram_size, len(vocabulary)\n",
        "      ))\n",
        "  )\n",
        "training_embeddings = vstack(training_embeddings)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aEOgwYKL1fy"
      },
      "source": [
        "## Training Fruit-Fly-Net ðŸ’ª"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9opqXdAgMUGQ"
      },
      "source": [
        "Let's instantiate our Fruit-Fly-Net now. We use $K=400$, $k=51$, and a learning rate of $1e-6$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5O-vES6PgNI6"
      },
      "source": [
        "model = FruitFlyNet(\n",
        "  input_dim=vocabulary_size*2,  # input dimension size (vocab_size * 2)\n",
        "  output_dim=400,               # output dimension size\n",
        "  k=51,                         # top k cells to be left active in output layer\n",
        "  lr=1e-6                       # learning rate (learning is performed internally)\n",
        ")\n",
        "model.to('gpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfNyWpZ2Muy4"
      },
      "source": [
        "For each epoch in our train loop, we shuffle our trainset and iterate over each batch. For each batch, we feed the inputs to the model. The weight update is performed interally. We also print out the loss for every 1000 batches and at the end of each epoch. We use a batch size of $32$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zf2BQC4WOalA"
      },
      "source": [
        "batch_size = 32\n",
        "\n",
        "loss = 0\n",
        "epochs = 10\n",
        "for epoch in range(epochs):\n",
        "  \n",
        "  # shuffle trainset\n",
        "  shuffled_idxs = xp.random.permutation(training_embeddings.shape[0])\n",
        "  training_embeddings = training_embeddings[shuffled_idxs]\n",
        "    \n",
        "  for batch_start in tqdm(range(0, training_embeddings.shape[0], batch_size)):\n",
        "    \n",
        "    # train step\n",
        "    input = training_embeddings[batch_start:batch_start+batch_size].toarray()\n",
        "    model(input, probabilities)\n",
        "    \n",
        "    # get loss\n",
        "    loss += bio_hash_loss(model.weights, input, probabilities)\n",
        "    \n",
        "    # print metrics every 1000 batches\n",
        "    if batch_start//batch_size % 1000 == 999:\n",
        "      print(f'epoch {epoch:2d} batch {batch_start//batch_size:4d}:\\t{loss/(batch_size*1000):.3f}')\n",
        "      loss = 0\n",
        "        \n",
        "  # print metrics after each epoch\n",
        "  print(f'epoch {epoch:2d} batch {batch_start//batch_size:4d}:\\t{loss/(batch_size*((training_embeddings.shape[0]//batch_size)%1000)):.3f}')\n",
        "  loss = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HnFljqMzNVbX"
      },
      "source": [
        "## Optional: Switching to a CPU runtime âš™ï¸"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzNO6kWZNs5Z"
      },
      "source": [
        "Before proceeding to the interactive demo in the next section, you may switch to a CPU runtime if you'd like. To do so, follow the succeeding steps."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNpItq5SODfh"
      },
      "source": [
        "1. Save the vocabulary, probabilities, and model weights. Make sure to download the files after savng."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJ62thNuKI3O"
      },
      "source": [
        "# save vocabulary\n",
        "with open('vocab.pkl', 'wb') as vocab_file:\n",
        "  pickle.dump(vocabulary, vocab_file)\n",
        "\n",
        "# save probabilities\n",
        "with open('prob.npy', 'wb') as prob_file:\n",
        "  xp.save(prob_file, cp.asnumpy(probabilities))\n",
        "\n",
        "# save model weights\n",
        "with open('weights.pkl', 'wb') as file:\n",
        "  pickle.dump(model.state_dict(), file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPDyv6gQOZ_4"
      },
      "source": [
        "2. Shut down this runtime by going to `Runtime > Factory Reset Runtime`. Then switch to a CPU runtime by going to `Runtime > Change Runtime Type > Hardware Accelerator > None`. After starting a new runtime, upload the `vocab.pkl`, `prob.npy`, and `weights.pkl` files to `/content/`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LMX-GLkO-kN"
      },
      "source": [
        "3. Redo some installations, imports, and downloads."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ago0z6EWPftI"
      },
      "source": [
        "pip install -U einops gradio spacy git+https://github.com/Ramos-Ramos/fruit-fly-net"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHrcqjgLP4Dk"
      },
      "source": [
        "!python -m spacy download en_core_web_sm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Qx7C8y7RElW"
      },
      "source": [
        "from einops import rearrange\n",
        "import cupy as cp\n",
        "import numpy as xp\n",
        "import numpy as np\n",
        "import gradio as gr\n",
        "import pandas as pd\n",
        "import spacy\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "from collections import OrderedDict\n",
        "import pickle\n",
        "\n",
        "from fruit_fly_net import FruitFlyNet, bio_hash_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYzT94r6QGTB"
      },
      "source": [
        "4. Redefine functions, reinstantiate classes, and load vocabulary, probabilities, and model weights."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_fofwxDQxe6"
      },
      "source": [
        "# load vocabulary\n",
        "with open('vocab.pkl', 'rb') as vocab_file:\n",
        "  vocabulary = pickle.load(vocab_file)\n",
        "  vocabulary_size = len(vocabulary)\n",
        "\n",
        "# load probabilities\n",
        "with open('prob.npy', 'rb') as prob_file:\n",
        "  probabilities = xp.load(prob_file)\n",
        "\n",
        "# tokenization functions and classes\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "def create_tokens_from_text(text):\n",
        "  \"\"\"Tokenizes text by:\n",
        "  - splitting with SpaCy\n",
        "  - ignoring punctuations, numbers, and stop words\n",
        "  \"\"\"\n",
        "  return [w.lemma_.lower() for w in nlp(text) \n",
        "          if not w.is_punct and \n",
        "          not w.like_num and \n",
        "          not w.lemma_.lower() in nlp.Defaults.stop_words]\n",
        "\n",
        "# reinstantiate model and load weights\n",
        "model = FruitFlyNet(\n",
        "  input_dim=vocabulary_size*2,  # input dimension size (vocab_size * 2)\n",
        "  output_dim=400,               # output dimension size\n",
        "  k=51,                         # top k cells to be left active in output layer\n",
        "  lr=1e-6                       # learning rate (learning is performed internally)\n",
        ")\n",
        "\n",
        "with open('weights.pkl', 'rb') as file:\n",
        "  model.load_state_dict(pickle.load(file))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9DhIikNUkkS"
      },
      "source": [
        "## Interactive demo âŒ¨ï¸"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxob92T5Uu6Z"
      },
      "source": [
        "Here we use Gradio to let you perform similarity search with static word embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yja7GbpIZKAI"
      },
      "source": [
        "The inputs for static embeddings differ from the input embeddings for training by ignoring context and only having a one-hot encoded target word in the remaining $N_{voc}$ dimensions of the vector. Let's start with a helper function that can create this type of embedding from a token and a vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6ahg1gIJeKU"
      },
      "source": [
        "def create_static_input_embedding_from_token(token, vocabulary):\n",
        "  token = (create_tokens_from_text(token)+[None])[0]\n",
        "  id = None if token not in vocabulary else vocabulary.index(token) + len(vocabulary)\n",
        "  input_embedding = xp.zeros(len(vocabulary)*2)\n",
        "  if id is not None:\n",
        "    input_embedding[id] = 1\n",
        "  return input_embedding"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FlY1eyheVaG"
      },
      "source": [
        "Now let's create static input embeddings for each token in our vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMSXk06fZSr7"
      },
      "source": [
        "static_input_embeddings = []\n",
        "for token in tqdm(vocabulary):\n",
        "  static_input_embeddings.append(\n",
        "      create_static_input_embedding_from_token(token, vocabulary)\n",
        "  )\n",
        "static_input_embeddings = xp.stack(static_input_embeddings)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZffk9CwZdL7"
      },
      "source": [
        "We then feed each input embedding into our model to create a static embedding for each token."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tfcqkVfYSVu"
      },
      "source": [
        "batch_size = 32\n",
        "model.eval()\n",
        "static_embeddings = []\n",
        "for batch_start in tqdm(range(0, static_input_embeddings.shape[0], batch_size)):\n",
        "  input = static_input_embeddings[batch_start:batch_start+batch_size]\n",
        "  static_embeddings.append(model(input, probabilities))\n",
        "static_embeddings = xp.concatenate(static_embeddings)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snykxGCre1aI"
      },
      "source": [
        "Now we can find the $n$ most similar words for a given input word (ex. \"fire\", \"wing\", \"night\").\n",
        "\n",
        "Have fun!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSwmnV6ray7J"
      },
      "source": [
        "def get_top_similar_tokens_with_scores(token, top_similar):\n",
        "  \n",
        "  token = (create_tokens_from_text(token)+[None])[0]\n",
        "  id = None if token not in vocabulary else vocabulary.index(token)\n",
        "  if id is None:\n",
        "    return {'out of vocabulary': 1.0}\n",
        "  \n",
        "  input_embedding = create_static_input_embedding_from_token(token, vocabulary)\n",
        "  input_embedding = rearrange(input_embedding, 'd -> () d')\n",
        "  \n",
        "  model.eval()\n",
        "  embedding = model(input_embedding, probabilities)\n",
        "  \n",
        "  similarities = cosine_similarity(\n",
        "      cp.asnumpy(embedding), cp.asnumpy(static_embeddings)\n",
        "  )\n",
        "  similarities = rearrange(similarities, '() i -> i')\n",
        "  \n",
        "  current_vocabulary = vocabulary\n",
        "  if id is not None:\n",
        "    similarities = np.concatenate((similarities[:id], similarities[id+1:]))\n",
        "    current_vocabulary = current_vocabulary[:id]+current_vocabulary[id+1:]\n",
        "\n",
        "  top_similar_ids = similarities.argsort(kind='stable')[-top_similar:].tolist()\n",
        "  top_similar_scores = similarities[top_similar_ids]\n",
        "  top_similar_tokens = [current_vocabulary[id] for id in top_similar_ids]\n",
        "  return OrderedDict(zip(top_similar_tokens, top_similar_scores))\n",
        "\n",
        "r = gr.inputs.Slider(1, 20, step=1, default=10)\n",
        "gr.Interface(fn=get_top_similar_tokens_with_scores, inputs=['text', r], outputs='label').launch()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}